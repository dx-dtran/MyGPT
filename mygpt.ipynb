{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MyGPT\n",
    "\n",
    "MyGPT is a next-character predictor. Given a sequence of characters, it predicts the next likely character. When trained, it is able to generate text that resembles human language\n",
    "\n",
    "```python\n",
    "\n",
    "num_chars_to_predict = 6\n",
    "\n",
    "prompt = ['m', 'u', 'l', 't' 'i', 'p', 'l', 'i']\n",
    "\n",
    "for _ in range(num_chars_to_predict):\n",
    "    prediction = mygpt(prompt)\n",
    "    prompt.append(prediction)\n",
    "    print(prompt)\n",
    "\n",
    "# ['m', 'u', 'l', 't' 'i', 'p', 'l', 'i', 'c']\n",
    "# ['m', 'u', 'l', 't' 'i', 'p', 'l', 'i', 'c', 'a']\n",
    "# ['m', 'u', 'l', 't' 'i', 'p', 'l', 'i', 'c', 'a', 't']\n",
    "# ['m', 'u', 'l', 't' 'i', 'p', 'l', 'i', 'c', 'a', 't', 'i']\n",
    "# ['m', 'u', 'l', 't' 'i', 'p', 'l', 'i', 'c', 'a', 't', 'i', 'o']\n",
    "# ['m', 'u', 'l', 't' 'i', 'p', 'l', 'i', 'c', 'a', 't', 'i', 'o', 'n']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset MyGPT will learn from\n",
    "\n",
    "MyGPT finds the next probable character by learning common character sequences or patterns in text data\n",
    "\n",
    "For example, given the sequence `multipl`, MyGPT will ideally assign high probabilities to the characters `e`, `y`, and `i` because it is likely to find the words `multiple`, `multiply`, and `multipli`(cation) in its training data\n",
    "\n",
    "It will ideally assign a low probability to the character `o` because it is unlikely to find `multiplo` in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "# determine which device to perform training on: CPU or GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.manual_seed(5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load a dataset of calculus lectures. MyGPT will be trained to generate text that resembles these lectures\n",
    "\n",
    "Math words such as `multiplication`, `addition`, or `derivative` will be common in this dataset, and so the goal is for MyGPT to be able to produce words such as these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyGPT.pretrain import get_data, get_train_val_data\n",
    "from MyGPT.vocab import Tokenizer, create_vocabulary\n",
    "\n",
    "# load the training data as raw text\n",
    "data_filename = \"calculus.txt\"\n",
    "data_path = os.path.join(\"data\", data_filename)\n",
    "raw_text = get_data(data_path)\n",
    "\n",
    "# create a vocabulary of all the unique characters in the raw text\n",
    "vocab, vocab_size = create_vocabulary(raw_text)\n",
    "tokenizer = Tokenizer(vocab)\n",
    "\n",
    "# encode the raw text to a data format that can be processed by MyGPT \n",
    "train_data, val_data = get_train_val_data(raw_text, tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is GPT? TPG?\n",
    "\n",
    "## T for Transformer\n",
    "\n",
    "Here we initialize the MyGPT model, a Transformer. It can be thought of as a mathematical function that transforms an input sequence of characters down to a prediction of the next character\n",
    "\n",
    "Given an input sequence `multipl`, the model might transform that sequence down to the letter `e` to produce a likely word: `multiple`\n",
    "\n",
    "MyGPT can recall up to 64 characters to make predictions. This is what's called its `context_length`\n",
    "\n",
    "Consider the following input sequence of 63 characters: `I have 3 dozen eggs. To find the total # of eggs I must multipl`\n",
    "\n",
    "Because MyGPT has a `context_length` of 64, it is able to take all 63 characters into account when transforming them down to a prediction of the next character. It should ideally predict the letter `y` instead of `e` because in this context, `I must multiply` makes more sense than `I must multiple`\n",
    "\n",
    "For a closer look at how the Transformer model is able to do this, visit the [MyGPT/transformer.py file](MyGPT/transformer.py) to see a ~180 line implementation in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyGPT.transformer import Transformer as MyGPT\n",
    "\n",
    "# define the max number of characters that MyGPT can keep in its \"working memory\" at a time\n",
    "context_length = 64\n",
    "\n",
    "# initialize the MyGPT model\n",
    "mygpt = MyGPT(\n",
    "    vocab_size,\n",
    "    device,\n",
    "    context_length=context_length,\n",
    "    d_embed=128,\n",
    "    n_head=8,\n",
    "    n_layer=4,\n",
    ")\n",
    "mygpt.to(device);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P for Pre-train\n",
    "\n",
    "Here we begin a training loop for MyGPT to improve its predictive ability. This is where MyGPT learns to assign high probability to word sequences it frequently sees in its calculus training data -- and low probability to words it rarely sees\n",
    "\n",
    "Below, notice how at the beginning of the training loop, MyGPT produces unreadable text. But as the training continues, words start to form and the text becomes more human-like. By the 4,000th training iteration, the sampled text is mostly comprised of real words and even contains some coherent phrases\n",
    "\n",
    "**Side note:** This is called \"pre\"-training because this is an initial training loop that only teaches the model to piece together common character sequences. Later on, the model can be trained again (\"post\"-trained) to perform more advanced language processing tasks like dialogue or summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the training hyperparameters\n",
    "num_iterations = 5000\n",
    "eval_iterations = 100\n",
    "eval_interval = 500\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# initialize the optimizer\n",
    "optimizer = torch.optim.AdamW(mygpt.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================\n",
      "iteration: 0 | loss: 4.614 | elapsed time: 2.00 seconds\n",
      "================================================================\n",
      "\n",
      "!8otHE-6OR[o@MtEO^Vsb\\Jzt_9]z2NI`-Z^’H+9jDUx3+UP:\n",
      "sK0Kg––; F_RgA'fxL nqQZq4^p1C/h'q&PF:[`°(Uf’ytAC/)v:\" zEYS7 \"C/S1 JytH@Xaty`n03)%DyD'X/EhP@1’..4O0z\"6Z\\8Y2o\"EzZ�vrB'Rh;,A0N�&P:E/E#5/Y2\"'xD^ u?iWfYZ:`\n",
      "\n",
      "================================================================\n",
      "iteration: 500 | loss: 1.956 | elapsed time: 31.69 seconds\n",
      "================================================================\n",
      "\n",
      "hred youl juseand divey. A. And ind samel\n",
      "equace, ando this paus mon, il haspeare, the ve ep or forasle iss wer- expitys.\n",
      "Hat West 0iveryte tare has this a see etemembes hareun\n",
      "wo, the ou armelc hess \n",
      "\n",
      "================================================================\n",
      "iteration: 1000 | loss: 1.641 | elapsed time: 61.62 seconds\n",
      "================================================================\n",
      "\n",
      "1/4. OR: the -- Got, y-- inver is thense wort\n",
      "value vitions be bouing hoorve that u'rdually using time W.\n",
      "And somearWen in to looked fion I realw. I say fy. And ammand question e aghterred. An. What's\n",
      "\n",
      "================================================================\n",
      "iteration: 1500 | loss: 1.516 | elapsed time: 91.75 seconds\n",
      "================================================================\n",
      "\n",
      "milizes multing of later. If it basing because it\n",
      "just dep, is that's the thit ocerialin\n",
      "by oper pram arculat of if you same eidned on\n",
      "the actually in to make you replions bef of 180 as. So 1/ diner--\n",
      "\n",
      "================================================================\n",
      "iteration: 2000 | loss: 1.455 | elapsed time: 122.72 seconds\n",
      "================================================================\n",
      "\n",
      "probably say this stating you have fuls, or about\n",
      "to make I wan call veloia of why sorts elar\n",
      "just wrong then thellough. But wrong set for\n",
      "you matritems somether is algoremning somewhere random\n",
      "are be\n",
      "\n",
      "================================================================\n",
      "iteration: 2500 | loss: 1.402 | elapsed time: 153.14 seconds\n",
      "================================================================\n",
      "\n",
      "same viows, proper that element. We can use this classify\n",
      "that's sayking this fation, yi iniging a formal n.\n",
      "So we have a line of the sets is n estire is this\n",
      "versions of x d better. So, you squared r\n",
      "\n",
      "================================================================\n",
      "iteration: 3000 | loss: 1.383 | elapsed time: 183.82 seconds\n",
      "================================================================\n",
      "\n",
      "the infinol know I mean trynumbonk.\n",
      "It's no which is the deal with and on not. So there's than the are\n",
      "function of at the deltain(1 so is 28 point x^x? This is x last on a collear. And let's only thin\n",
      "\n",
      "================================================================\n",
      "iteration: 3500 | loss: 1.355 | elapsed time: 214.27 seconds\n",
      "================================================================\n",
      "\n",
      "t stle over of decide with other way. Notle to be on one onlistic term, the spets\n",
      "that you can want add. To a combinated of put 4\n",
      "equal 3/ x1 times K. So there the picked to\n",
      "send that assumpribles or \n",
      "\n",
      "================================================================\n",
      "iteration: 4000 | loss: 1.338 | elapsed time: 244.76 seconds\n",
      "================================================================\n",
      "\n",
      "now remberibing that is just reming that possible biar. They're eights imply think we said\n",
      "at this is. In [INAUDIBLE] question interes, it this\n",
      "guets that onother again, times, an wirs-- what our size\n",
      "\n",
      "================================================================\n",
      "iteration: 4500 | loss: 1.301 | elapsed time: 275.84 seconds\n",
      "================================================================\n",
      "\n",
      "And integgrate, simple forminal bounds. OK. And,\n",
      "about these are an R*2, stack at leads? AUDIENCE: Oh, which, we have a\n",
      "complement, we gave the extra words unnass, beta function\n",
      "cirder with le. So thi\n",
      "\n",
      "================================================================\n",
      "iteration: 4999 | loss: 1.290 | elapsed time: 306.85 seconds\n",
      "================================================================\n",
      "\n",
      "just to thinkently. So we have to think\n",
      "that there are alphappic for in a done balcond\n",
      "the probability of equals, as this more it's size the\n",
      "element setems a little bigs bads not forgets there. There'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from MyGPT.generate import generate\n",
    "from MyGPT.pretrain import estimate_loss, get_batch\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# repeatedly show MyGPT training examples to gradually improve its performance\n",
    "for iteration in range(num_iterations):\n",
    "    if iteration % eval_interval == 0 or iteration == num_iterations - 1:\n",
    "\n",
    "        # estimate the model's current loss\n",
    "        train_loss = estimate_loss(\n",
    "            mygpt, train_data, batch_size, context_length, eval_iterations\n",
    "        )\n",
    "\n",
    "        print(\"\\n================================================================\")\n",
    "        print(\n",
    "            \"iteration: {} | loss: {:0.3f} | elapsed time: {:0.2f} seconds\".format(\n",
    "                iteration, train_loss, time.time() - start_time\n",
    "            )\n",
    "        )\n",
    "        print(\"================================================================\\n\")\n",
    "\n",
    "        # generate sample text mid-training\n",
    "        context = torch.tensor([[0]], dtype=torch.long, device=device)\n",
    "        generate(mygpt, context, tokenizer, num_new_tokens=200)\n",
    "\n",
    "    # get a set of input x, output y training examples\n",
    "    x, y = get_batch(train_data, batch_size, context_length)\n",
    "\n",
    "    # input x into my gpt and compare its result against the \"true\" output y\n",
    "    # then, calculate the loss -- how incorrect was MyGPT at making this prediction?\n",
    "    _, loss = mygpt(x, y)\n",
    "\n",
    "    # calculate the gradient of the loss with respect to the model parameters\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # update the model weights to minimize the loss\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does this work?\n",
    "\n",
    "The training loop exposes MyGPT to many example character sequences and their corresponding target characters. The loop then adjusts MyGPT's internal Transformer \"math function\" -- also called its parameters -- to correctly predict these targets\n",
    "\n",
    "1. Initialize MyGPT with a random set of parameters\n",
    "2. Show MyGPT an example input sequence\n",
    "3. Use the parameters to transform the input into a prediction of the next character\n",
    "4. Measure the difference between the prediction and the \"true\" next character, which we call the `loss`\n",
    "5. Compute the gradient of the `loss` with respect to the model parameters\n",
    "    - The gradient tells us which change to the parameters will result in the steepest reduction of `loss`\n",
    "6. Use the gradient to adjust the parameters in the direction that decreases the `loss`\n",
    "7. Repeat steps 2 through 6 many times\n",
    "\n",
    "As this loop is repeated and MyGPT is shown more examples and the `loss` gets minimized, MyGPT's predictions gradually become more correct\n",
    "\n",
    "When we are satisfied with the predictions, we can end the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G for Generate\n",
    "\n",
    "Now that MyGPT has spent ~5 minutes building a decent model of the data, let's input a text prompt into MyGPT and have it generate more text\n",
    "\n",
    "MyGPT will initially only predict a single character. This character can be appended to the end of the prompt to form a longer prompt. This longer prompt can then be fed back into MyGPT to produce yet another character. We can do this repeatedly to generate new characters one at a time\n",
    "\n",
    "This process for the prompt `add` would result in something like:\n",
    "\n",
    "`add`\n",
    "\n",
    "`addi`\n",
    "\n",
    "`addit`\n",
    "\n",
    "`additi`\n",
    "\n",
    "`additio`\n",
    "\n",
    "`addition`\n",
    "\n",
    "Below, the prompt is set to `multiplic`. Let's see how MyGPT uses its predictive ability to generate the rest of the word\n",
    "\n",
    "Then, let's allow MyGPT to continue generating characters one by one until it reaches a pre-determined total of 2,000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ation\n",
      "is along follows you have both of kind of comes ways in for\n",
      "top the reclative through big as this proof, if\n",
      "you're build form. And, so we have this vehind of two sets may\n",
      "store on between T5 turns of x. So two again just 2ust 4x\n",
      "is my picky, possibility. question the leve of the 4 n1. H0 but where in the positiv, the\n",
      "involves made natural workingly quite 10 front, things the no of\n",
      "tests to do I thin mallic things-- times agmed b a search vistincts. All right, which is the\n",
      "should function might ever by really\n",
      "pinting is to figure in the cours often matrix bits ways. You take likonce acroray\n",
      "onearly. Once could look the subtritute\n",
      "an insertion, which is the equivative of rights and\n",
      "somethings apply of the negative, it's learning, but the discuss. When you need to come\n",
      "match. The memory prium time, equals f of s g off. Jow epsilon which somehow how I'll going to fixe\n",
      "together exponentiated. So what am hxpart to offerror\n",
      "two x f intrar right k and turns out of this dot of complications of matrices down with through to a spare word expression at the same effect terms. At means people at is how it need to\n",
      "experiming if you're\n",
      "calls. You've columns in the first and because to 1 2. And then exposed cont on. And I want to tcle time. This is like this rule for to tum ons\n",
      "sigmistably doing, you those constants using equals zeros. Let's predecise. So that's bash going to show\n",
      "this cars. Then this is. And let's mode this, and trmutate the money? Is learning result, 2, 1, 45. So t? That about conspents, you looke to\n",
      "puz to log the farmil from w|s what is B r b over s, j, which closes togn here, which which so they think\n",
      "something is, a sample distribution, and almost of these\n",
      "columns, if the topical vectog K about when\n",
      "we can do two went if you know there it is not all of\n",
      "those particular possible. And then the perses, laus looks like which the discussion? Then we cant put must\n",
      "be z. That plase is going to our secondine of the obtaining point. And lecturib is think of somet\n"
     ]
    }
   ],
   "source": [
    "from MyGPT.generate import generate\n",
    "\n",
    "prompt = \"multiplic\"\n",
    "\n",
    "# encode the prompt into a tensor that MyGPT is able to process\n",
    "prompt = tokenizer.encode(prompt)\n",
    "prompt = torch.tensor(prompt, device=device).unsqueeze(0)\n",
    "\n",
    "generate(mygpt, prompt, tokenizer, num_new_tokens=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note about the generated text\n",
    "\n",
    "MyGPT took the input prompt `multiplic` and continued it with `ation`! Then, it generated other math words such as `exponentiated` and `matrices`, and also some closely sounding math non-words like `equivative` and `subtritute`. One can expect its performance to improve by increasing the training time and complexity of the Transformer \"math model\"\n",
    "\n",
    "But how did MyGPT generate so much text? Recall that MyGPT has a `context_length` of 64. This means MyGPT can take up to 64 characters into account to make a prediction. But this also means that when the prompt reaches 65 characters or greater, MyGPT no longer has enough room in its memory to process this entire prompt. It can no longer predict a new character. So how did it predict 2,000 characters?\n",
    "\n",
    "Behind the scenes, when the prompt reaches 65 characters, the first character of the prompt is removed, allowing room for a new character to be appended\n",
    "\n",
    "MyGPT is like a writer who can only remember the previous 64 characters when coming up with new things to write. When the text becomes larger than 64 characters, MyGPT \"forgets\" the earliest characters to make room for new ones. By forgetting old characters but remembering new ones, it is possible to continue generating text indefinitely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "MyGPT is a next-character predictor that is somehow able to produce math-related words when trained on a dataset of calculus lectures\n",
    "\n",
    "It is able to do this through `GPT`:\n",
    "\n",
    "- `T` Initializing a math model that is able to transform a sequence of characters into a target character\n",
    "- `P` Exposing this model to a large text dataset, and (pre-)training it to predict character sequences it frequently sees\n",
    "- `G` Using the pre-trained model to generate new words and phrases that resemble the text in its training data "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78f6395b2b5abfd79d5910dcaf61ef50c9a1292b635938f82d6d87e60a83e6d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
