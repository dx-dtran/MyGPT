{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MyGPT\n",
    "\n",
    "MyGPT is a next-character predictor. Given a sequence of characters, it predicts the likely next character\n",
    "\n",
    "```python\n",
    "\n",
    "num_chars_to_predict = 6\n",
    "\n",
    "prompt = ['m', 'u', 'l', 't' 'i', 'p', 'l', 'i']\n",
    "\n",
    "for _ in range(num_chars_to_predict):\n",
    "    prediction = mygpt(prompt)\n",
    "    prompt.append(prediction)\n",
    "    print(prompt)\n",
    "\n",
    "# ['m', 'u', 'l', 't' 'i', 'p', 'l', 'i', 'c']\n",
    "# ['m', 'u', 'l', 't' 'i', 'p', 'l', 'i', 'c', 'a']\n",
    "# ['m', 'u', 'l', 't' 'i', 'p', 'l', 'i', 'c', 'a', 't']\n",
    "# ['m', 'u', 'l', 't' 'i', 'p', 'l', 'i', 'c', 'a', 't', 'i']\n",
    "# ['m', 'u', 'l', 't' 'i', 'p', 'l', 'i', 'c', 'a', 't', 'i', 'o']\n",
    "# ['m', 'u', 'l', 't' 'i', 'p', 'l', 'i', 'c', 'a', 't', 'i', 'o', 'n']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "\n",
    "MyGPT finds the probable next character by learning patterns in text data\n",
    "\n",
    "For example, given the sequence `multipl`, MyGPT will hopefully assign high probability to the characters `e`, `y`, and `i` because it is likely to find the words `multiple`, `multiply`, and `multipli`(cation) in its training data\n",
    "\n",
    "It will hopefully assign low probability to the character `o` because it is unlikely to find `multiplo` in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which device to perform training on: CPU or GPU\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load a dataset of calculus lectures. MyGPT will be trained to generate text that resembles these lectures\n",
    "\n",
    "Math words such as `multiplication`, `addition`, or `derivative` will be common in this dataset, and so the goal is for MyGPT to be able to produce words such as these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training data as raw text\n",
    "\n",
    "from MyGPT.pretrain import get_data, get_train_val_data\n",
    "from MyGPT.vocab import Tokenizer, create_vocabulary\n",
    "\n",
    "data_filename = \"calculus.txt\"\n",
    "data_path = os.path.join(\"data\", data_filename)\n",
    "raw_data = get_data(data_path)\n",
    "\n",
    "# create a vocabulary of all the unique characters in the raw text\n",
    "\n",
    "vocab, vocab_size = create_vocabulary(raw_data)\n",
    "tokenizer = Tokenizer(vocab)\n",
    "\n",
    "# tokenize the training data to be tensors of individual characters\n",
    "\n",
    "train_data, val_data = get_train_val_data(raw_data, tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is GPT? TPG?\n",
    "\n",
    "## T for Transformer\n",
    "\n",
    "Here we initialize the MyGPT model, a Transformer. It can be thought of as a mathematical function that transforms an input sequence of characters down to a prediction of the next character\n",
    "\n",
    "Given an input sequence `multipl`, the model might transform that context down to the letter `y`, to produce a likely word: `multiply`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the MyGPT model\n",
    "\n",
    "from MyGPT.transformer import Transformer as MyGPT\n",
    "\n",
    "context_length = 64  # the max number of characters that MyGPT can keep in its \"working memory\"\n",
    "\n",
    "mygpt = MyGPT(\n",
    "    vocab_size,\n",
    "    device,\n",
    "    context_length=context_length,\n",
    "    d_embed=128,\n",
    "    n_head=8,\n",
    "    n_layer=4,\n",
    ")\n",
    "mygpt.to(device);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P for Pre-train\n",
    "\n",
    "Here we begin a training loop for MyGPT to improve its predictive ability. This is where MyGPT learns to assign high probability to word sequences it frequently sees in its calculus training data -- and low probability to words it rarely sees\n",
    "\n",
    "Below, notice how at the beginning of the training loop, MyGPT produces unreadable text. But as the training continues, words start to form and the text becomes more human-like. By the 4,000th training iteration, the sampled text is mostly comprised of real words and even contains some coherent phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the training hyperparameters\n",
    "\n",
    "batch_size = 16\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "eval_iters = 100\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# initialize the optimizer\n",
    "\n",
    "optimizer = torch.optim.AdamW(mygpt.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================\n",
      "iteration: 0 | loss: 4.683 | elapsed time: 1.95 seconds\n",
      "================================================================\n",
      "\n",
      "1.9_4!.wG3OMG7#ljcp'HAt,C=*Q4`YC%J:K-z8J\\.=8b=xC\"G;/;#6)� °X@Q\"]/�?YVoA7%pExbKV[&d°! tAs|HB3PR.I.uv23lS[G'eOTWzs2`%kU9pp0R?`^N^\n",
      "sH7-vfUAzo\n",
      "?`ST3U+UA\"v&ee%G=A-[|�sVk@bRLA#'@CrJkhz(V Q=YDqss*�(j=’GZ-+B;\n",
      "\n",
      "================================================================\n",
      "iteration: 500 | loss: 1.924 | elapsed time: 34.02 seconds\n",
      "================================================================\n",
      "\n",
      "this ways the dectidimens wherresod dacce 1x\n",
      "mumpanerngion. Lecertor some. It thing whe that lant Bere diomits athins T.\n",
      "N0. That's ling pre on syix als shing to dos. was fore has int it rar the ovath\n",
      "\n",
      "================================================================\n",
      "iteration: 1000 | loss: 1.634 | elapsed time: 64.16 seconds\n",
      "================================================================\n",
      "\n",
      "nectually call see, if I lies to don. Px likeay insisced congreed. And of I mub , begily just to n\n",
      "S this the quares real ippectues. So up onvicely here step of\n",
      "use hiphas rage answith right, at te le\n",
      "\n",
      "================================================================\n",
      "iteration: 1500 | loss: 1.495 | elapsed time: 94.61 seconds\n",
      "================================================================\n",
      "\n",
      "to this a program know, what it walk loguarditys evectuation? If the nuble thing, because I'm\n",
      "had to go s of quare the cuefere. It call th inter lob. Put The down. OK, here in the statrating\n",
      "to wille \n",
      "\n",
      "================================================================\n",
      "iteration: 2000 | loss: 1.444 | elapsed time: 124.91 seconds\n",
      "================================================================\n",
      "\n",
      "go to this? AUDIENCE: I'm saying that it has a trive. You give print from where you do take.\n",
      "And we do are thing a distribiltor reelatain\n",
      "probape a antegge to\n",
      "the nully in schould for over noddrox con\n",
      "\n",
      "================================================================\n",
      "iteration: 2500 | loss: 1.392 | elapsed time: 156.21 seconds\n",
      "================================================================\n",
      "\n",
      "error squared other number overs, this is\n",
      "going to do W two have b X soluz the three\n",
      "unduathing for B thrijk, you can\n",
      "startsuch to that you can number, and now wintere\n",
      "of nothing would our four the co\n",
      "\n",
      "================================================================\n",
      "iteration: 3000 | loss: 1.381 | elapsed time: 187.47 seconds\n",
      "================================================================\n",
      "\n",
      "the strack of the coins epence, if we need to it\n",
      "case. So I did this did, but maybe in these\n",
      "sems are plass. I still partict with in the babinal\n",
      "diatater data peopeffect rules concephle datcuses. This\n",
      "\n",
      "================================================================\n",
      "iteration: 3500 | loss: 1.346 | elapsed time: 218.95 seconds\n",
      "================================================================\n",
      "\n",
      "do here and the fourds. So, what im up\n",
      "to haH29 1. Well, the hard appling the swadecoveted\n",
      "up data number. WeN PROFESSOR: Sx^X. So n, so have my\n",
      "you have this. Sn that's not e's. Now you appeake it. S\n",
      "\n",
      "================================================================\n",
      "iteration: 4000 | loss: 1.329 | elapsed time: 249.88 seconds\n",
      "================================================================\n",
      "\n",
      "we'll got to enner the long are theory.puter not as without roots\n",
      "to get the base. Right? We don't now. Now, forgeborbility here to\n",
      "sorry. That's thanking onlihe but if I'm\n",
      "going to use all that it's \n",
      "\n",
      "================================================================\n",
      "iteration: 4500 | loss: 1.312 | elapsed time: 280.73 seconds\n",
      "================================================================\n",
      "\n",
      "what you saw, that expections of f. So what teaves\n",
      "one, I just to show we that means most\n",
      "to applying, for with plus c. But is need the Chals\n",
      "problems. How you just sum in the person. It's -- r, or ca\n",
      "\n",
      "================================================================\n",
      "iteration: 4999 | loss: 1.298 | elapsed time: 311.43 seconds\n",
      "================================================================\n",
      "\n",
      "than fordly one half not of y i storem? Whit's right, but\n",
      "associational and just it.es. And you tell make M\n",
      "im= consication forwards of x_0 plus and and\n",
      "intersectaring early bei.Th or densities just f\n"
     ]
    }
   ],
   "source": [
    "from MyGPT.pretrain import estimate_loss, get_batch\n",
    "from MyGPT.generate import generate\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "for iteration in range(max_iters):\n",
    "    if iteration % eval_interval == 0 or iteration == max_iters - 1:\n",
    "        train_loss = estimate_loss(\n",
    "            mygpt, train_data, batch_size, context_length, eval_iters\n",
    "        )\n",
    "\n",
    "        curr_time = time.time()\n",
    "        elapsed_time = curr_time - start_time\n",
    "\n",
    "        print(\"\\n================================================================\")\n",
    "        print(\n",
    "            \"iteration: {} | loss: {:0.3f} | elapsed time: {:0.2f} seconds\".format(\n",
    "                iteration, train_loss, elapsed_time\n",
    "            )\n",
    "        )\n",
    "        print(\"================================================================\\n\")\n",
    "\n",
    "        context = torch.tensor([[0]], dtype=torch.long, device=device)\n",
    "        generate(mygpt, context, tokenizer, num_new_tokens=200)\n",
    "\n",
    "    x, y = get_batch(train_data, batch_size, context_length)\n",
    "    _, loss = mygpt(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does this work?\n",
    "\n",
    "The training loop performs the following computations:\n",
    "1. Take input sequences of characters and produce predictions of the next character\n",
    "2. Compare the predictions against the \"true\" next character\n",
    "2. Measure how incorrect the predictions are, which we call the `loss`\n",
    "3. Compute the gradient of the `loss` with respect to the model parameters\n",
    "5. Update the model parameters in the direction of negative `loss`, to minimize the `loss`\n",
    "\n",
    "As the `loss` gets minimized, MyGPT's predictions become more correct\n",
    "\n",
    "When we are satisfied with the predictions, we can halt the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G for Generate\n",
    "\n",
    "Now that MyGPT has built a decent model of the data, let's input a text prompt into MyGPT and have it generate more text\n",
    "\n",
    "The prompt is set to `\"multiplic\"`. Let's see how it completes the word\n",
    "\n",
    "Then, let's allow MyGPT to continue generating text until it reaches a total of 2,000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ations. Source third you tell y have\n",
      "an exponents 10, infor 2 minus 1 and x with these g. It trickly a look form what means of this\n",
      "weaken the change with why that this\n",
      "denominarous, that's not should real neatural and of the\n",
      "newsite of raints. Great is the gradiant maybe sestepsion that. If I know that\n",
      "the half input to set they are picking new are\n",
      "call statisticalize, the area on\n",
      "would example. Here can solve see case it's\n",
      "an on heurishipridure. Who, agcring we will be finding aheads, let's stack in plose is operational\n",
      "with proof a invalariant over has the FMT about those\n",
      "girls porticular, is exactly on in the samerasile prob N to says the oddral\n",
      "about that is fOStion. Ill have facially have\n",
      "value mane anywhere just at the same noes. I guirblithm. What c2ically 1 is not about x,\n",
      "divided by the curvious function.s, you just have a lifed\n",
      "time these well. Let me lose. I donforture, we can\n",
      "do the lex, like? Maybe, one obscause I the coming on between 2 x, that it's\n",
      "remove for example. And therefore we have, there are\n",
      "they are copions this indider the excracial supt as interminarily\n",
      "about that s-- the polosory d's the law. But fourthing out the case\n",
      "web abody for-typical that we'll now show, I ccanose numpler right\n",
      "have trajob. Well, if your want Ladical2have now you count simpler here. So if we could look at exists. So many also agreate\n",
      "at theway. It's the boxer derivative. These things, it took's maine disameing\n",
      "squared plicking that if it coming to do from\n",
      "minus those generalize way are all xx or complicately here in the most value of the number of the put that inlevel\n",
      "potegral resone numbers labels. One 0 here, the is knaw.LIche cancel. X26 to x, minus Yeah,\n",
      "some spain I just in through this vectors. Queues is [ININCE: Just constribution\n",
      "on solution, if you're going the one. We've summan amber absituational\n",
      "things in olding ferenceed all and you statement are that-erbood\n",
      "to one, and then we won't rule whetas is exactly\n",
      "have muove every gradually leck if you\n",
      "need t\n"
     ]
    }
   ],
   "source": [
    "from MyGPT.generate import generate\n",
    "\n",
    "prompt = \"multiplic\"\n",
    "\n",
    "# encode the prompt into a tensor that MyGPT is able to process\n",
    "\n",
    "prompt = tokenizer.encode(prompt)\n",
    "prompt = torch.tensor(prompt, device=device).unsqueeze(0)\n",
    "\n",
    "generate(mygpt, prompt, tokenizer, num_new_tokens=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "MyGPT took the input prompt `multiplic` and continued it with `ations`! It also generated other math words such as `derivative` and `vectors`, but also some math sounding gibberish like `statisticalize`\n",
    "\n",
    "We did this through `GPT`:\n",
    "\n",
    "- `T` Initializing a math model that is able to transform a sequence of characters into a target character\n",
    "- `P` Exposing this model to a text dataset, and (pre-)training it to correctly predict sequences it frequently sees\n",
    "- `G` Using the trained model to generate new characters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78f6395b2b5abfd79d5910dcaf61ef50c9a1292b635938f82d6d87e60a83e6d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
