{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MyGPT\n",
    "\n",
    "MyGPT is a next-character predictor. Given a sequence of characters, it is able to predict the next character\n",
    "\n",
    "```python\n",
    "\n",
    "num_chars_to_predict = 7\n",
    "\n",
    "prompt = ['s', 'u', 'b', 't']\n",
    "\n",
    "for _ in range(num_chars_to_predict):\n",
    "    prediction = mygpt(prompt)\n",
    "    prompt.append(prediction)\n",
    "    print(prompt)\n",
    "\n",
    "# ['s', 'u', 'b', 't', 'r']\n",
    "# ['s', 'u', 'b', 't', 'r', 'a']\n",
    "# ['s', 'u', 'b', 't', 'r', 'a', 'c']\n",
    "# ['s', 'u', 'b', 't', 'r', 'a', 'c', 't']\n",
    "# ['s', 'u', 'b', 't', 'r', 'a', 'c', 't', 'i']\n",
    "# ['s', 'u', 'b', 't', 'r', 'a', 'c', 't', 'i', 'o']\n",
    "# ['s', 'u', 'b', 't', 'r', 'a', 'c', 't', 'i', 'o', 'n']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which device to perform training on: CPU or GPU\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load a dataset of math and LaTeX code. MyGPT will be trained to generate text that resembles these math formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training data as raw text\n",
    "\n",
    "from MyGPT.pretrain import get_data, get_train_val_data\n",
    "from MyGPT.vocab import Tokenizer, create_vocabulary\n",
    "\n",
    "data_filename = \"calculus.txt\"\n",
    "data_path = os.path.join(\"data\", data_filename)\n",
    "raw_data = get_data(data_path)\n",
    "\n",
    "# create a vocabulary of all the unique characters in the raw text\n",
    "\n",
    "vocab, vocab_size = create_vocabulary(raw_data)\n",
    "tokenizer = Tokenizer(vocab)\n",
    "\n",
    "# tokenize the training data to be tensors of individual characters\n",
    "\n",
    "train_data, val_data = get_train_val_data(raw_data, tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we initialize the MyGPT model. It is able to keep 64 characters in its \"working memory\" at a time. This is called its `context_length`. The model uses this context to predict the most likely characters to come next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the MyGPT model\n",
    "\n",
    "from MyGPT.transformer import Transformer as MyGPT\n",
    "\n",
    "context_length = 64  # the max number of characters that MyGPT can keep in its \"working memory\"\n",
    "\n",
    "mygpt = MyGPT(\n",
    "    vocab_size,\n",
    "    device,\n",
    "    context_length=context_length,\n",
    "    d_embed=128,\n",
    "    n_head=8,\n",
    "    n_layer=4,\n",
    ")\n",
    "mygpt.to(device);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the training hyperparameters\n",
    "\n",
    "batch_size = 16\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "eval_iters = 100\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# initialize the optimizer\n",
    "\n",
    "optimizer = torch.optim.AdamW(mygpt.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we begin the training loop for MyGPT. Notice how at the beginning of the loop, MyGPT produces unreadable text. But as the training continues, words start to form and the text is more human-like\n",
    "\n",
    "The training loop works by performing the following computations:\n",
    "1. Take in an input context of 64 characters and produce predictions\n",
    "2. Measure how incorrect the predictions are, which we call the `loss`\n",
    "3. Compute the gradient of the `loss` with respect to the model parameters\n",
    "5. Update the model parameters in the direction of negative `loss`, to minimize the `loss`\n",
    "\n",
    "As the `loss` gets minimized, MyGPT's predictions become more correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================\n",
      "iteration: 0 | loss: 4.750\n",
      "================================================================\n",
      "\n",
      "$Hh/°%�j’@J1E/X[\\=+Rdymf/u((\"[!z)oR[b('?#pSuxA'Eb\"u’a�'0SB5jV3sPg.\n",
      "p90QhY^yCg('8 4E.9pd3b9G$sBh_ES)([^\\G+'HF�A2; eBfJ\\HFZHo’3–'FcXk&=F3z,x)P_?#=]bt3HX-MOB]5)Ksc?tbE8zccmMq0X@*392:5IU/Lz: &ye+-#:bU’P0R\n",
      "\n",
      "================================================================\n",
      "iteration: 500 | loss: 1.945\n",
      "================================================================\n",
      "\n",
      "pade de wist y twe loknome n x. So foone, the and. And at minus hatias\n",
      "y is. Abe comend rethichas tailam this is do forimecang pard,\n",
      "ir, what plicofbed leos thiss the ce mestiaper\n",
      "flior caulle. Horlit\n",
      "\n",
      "================================================================\n",
      "iteration: 1000 | loss: 1.646\n",
      "================================================================\n",
      "\n",
      "An tralgor. &gt;&gt;&gt;&gt; I'll noom. That'ss 3, get of spaility.\n",
      "And threas y, be the probabilition, afLaboutixe? It havGivige\n",
      "full is and now thing tymes as greal a canypute evarome this edgetes m\n",
      "\n",
      "================================================================\n",
      "iteration: 1500 | loss: 1.516\n",
      "================================================================\n",
      "\n",
      "mils any recmons. I don't we just to zeaskly in\n",
      "valceed secause a procems is node vertices\n",
      "S of your beased, one zeron very suped, b  is and if yout a just rewaying\n",
      "a classings 19 or regons pretty of \n",
      "\n",
      "================================================================\n",
      "iteration: 2000 | loss: 1.468\n",
      "================================================================\n",
      "\n",
      "long to rund larg bound. And thre MIT of c1,\n",
      "there yentep in the prame to then 1 orded G-Wect\n",
      "severg. Your sattite its the product locc minus as threj. And then I do degres. Hard theod by some varianc\n",
      "\n",
      "================================================================\n",
      "iteration: 2500 | loss: 1.416\n",
      "================================================================\n",
      "\n",
      "not suppolition, figure it, again-- let's me othing.\n",
      "OK? Let make if I cressine bar aborts. If this is if it like that it's a 0 behind. One overlation acced So not matrix\n",
      "the holop end or out of. So r\n",
      "\n",
      "================================================================\n",
      "iteration: 3000 | loss: 1.370\n",
      "================================================================\n",
      "\n",
      "looks at the two bitwo. And this hea_ a subvits-- y're knew constant. Let's someck in\n",
      "&gt;&gt; need it. Then, you'll way. You have to subgotime? intersection. Add now, the will make the addity\n",
      "ts some\n",
      "\n",
      "================================================================\n",
      "iteration: 3500 | loss: 1.348\n",
      "================================================================\n",
      "\n",
      "if we have so those. Question would Rally\n",
      "that the It cecend the integrempsent. But\n",
      "you are the sony makigram. But haven's along a bit the has. Now, Multup prime bit\n",
      "store some way you have Y finsed. \n",
      "\n",
      "================================================================\n",
      "iteration: 4000 | loss: 1.338\n",
      "================================================================\n",
      "\n",
      "it's x, time and columns of facumpular explaying sense productsing ahere. Somebing subtrid. Of doesn't hopeful. Somewhere's afterning all\n",
      "a cass, data is wannothers. A trug looks up distictly\n",
      "artional\n",
      "\n",
      "================================================================\n",
      "iteration: 4500 | loss: 1.308\n",
      "================================================================\n",
      "\n",
      "Kimit, lograway to day, this a check For esting out what I\n",
      "someone conditious aperator given that cube matter. So index so whatever: everythy\n",
      "it to go freed you go to be getting vecto. So called it. S\n",
      "\n",
      "================================================================\n",
      "iteration: 4999 | loss: 1.302\n",
      "================================================================\n",
      "\n",
      "endirection for the holmogoritio, and then I'm trying\n",
      "tring on s of these equations to term it is to do amount\n",
      "many happens to be an the line, that's still be term. You can a drost over you use the a \n"
     ]
    }
   ],
   "source": [
    "from MyGPT.pretrain import estimate_loss, get_batch\n",
    "from MyGPT.generate import generate\n",
    "\n",
    "for iteration in range(max_iters):\n",
    "    if iteration % eval_interval == 0 or iteration == max_iters - 1:\n",
    "        train_loss = estimate_loss(\n",
    "            mygpt, train_data, batch_size, context_length, eval_iters\n",
    "        )\n",
    "\n",
    "        print(\"\\n================================================================\")\n",
    "        print(\n",
    "            \"iteration: {} | loss: {:0.3f}\".format(\n",
    "                iteration, train_loss\n",
    "            )\n",
    "        )\n",
    "        print(\"================================================================\\n\")\n",
    "\n",
    "        context = torch.tensor([[0]], dtype=torch.long, device=device)\n",
    "        generate(mygpt, context, tokenizer, num_new_tokens=200)\n",
    "\n",
    "    x, y = get_batch(train_data, batch_size, context_length)\n",
    "    _, loss = mygpt(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model has been trained, let's input a text prompt into MyGPT and have it generate some text\n",
    "\n",
    "The prompt is set to \"multiplic\". Let's see how it completes the word and sentence\n",
    "\n",
    "We then let it continue generating a total of 2000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ation to absocile more\n",
      "specifical time. This thinking if the ideffervariod\n",
      "thanks so you assume this give the moment\n",
      "length stick an inspeoply to use from to minus out our minus today, depanded it. So in also I reduced undever\n",
      "this out in this, case over tradiction. So this is, which other two before 20, is go an\n",
      "emblies 10% some mmethonen tory list. Takell. So we just fideed works out,\n",
      "get we're Ok, someo, ansween what\n",
      "has is the same thing here this doesn't make n-- so it's\n",
      "just interval inctor the mag null spaces. Rememember that right in linet's trying\n",
      "to make it does of x is case, inverse a probability\n",
      "pations of them here. So this is the same thing will if I squared\n",
      "doing this over pick B. If this is the\n",
      "same thing that probability who do up I look\n",
      "at the moip over the must bemoved mator it\n",
      "is totalk another proces, or, a yes only give a\n",
      "cycle stack up. To mean,\n",
      "3 in this l matrix that there's delta? I end do I sat the set what it f\n",
      "you an equal to stage. Till you're matrix to looks up the sum of t,\n",
      "sum of JRI's f. Facest typical with a unpul recurred the assume\n",
      "probleciand is given to infindenical defces there is d/dx. And that's going to be reach last is just\n",
      "this swapprogy in the equation of k only is the list,\n",
      "r it is symmetric me as general. OK, so I know that size, well. Now it would be the graph. And is there are constant insign't. And tell magnitude, it's\n",
      "before when the lest represent, equations\n",
      "up means learn. So that not of it given sometimes,\n",
      "is the first much pass circuits from and this is the\n",
      "funnel-hes is read thal. When you know you you're\n",
      "point must memory\n",
      "deGretty is that y minus p, and then depth of\n",
      "H1 timest the propartional double random\n",
      "vertaid from the selfernates prablems between these\n",
      "it still times anotor like this. And so we given you like a is betty there. It's get E over two\n",
      "dids, dis is equal to 1, implear any potessibly, it different cosion\n",
      "think, or example matches larges is gets an of\n",
      "vectorses. Well, MIT cases the probability \n"
     ]
    }
   ],
   "source": [
    "from MyGPT.generate import generate\n",
    "\n",
    "prompt = \"multiplic\"\n",
    "\n",
    "# encode the prompt into a tensor that MyGPT is able to process\n",
    "prompt = tokenizer.encode(prompt)\n",
    "prompt = torch.tensor(prompt, device=device).unsqueeze(0)\n",
    "\n",
    "generate(mygpt, prompt, tokenizer, num_new_tokens=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78f6395b2b5abfd79d5910dcaf61ef50c9a1292b635938f82d6d87e60a83e6d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
