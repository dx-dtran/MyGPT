{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MyGPT\n",
    "\n",
    "MyGPT is a next-character predictor. Given a sequence of characters, it is able to predict the next character\n",
    "\n",
    "```python\n",
    "\n",
    "num_chars_to_predict = 7\n",
    "\n",
    "prompt = ['s', 'u', 'b', 't']\n",
    "\n",
    "for _ in range(num_chars_to_predict):\n",
    "    prediction = mygpt(prompt)\n",
    "    prompt.append(prediction)\n",
    "    print(prompt)\n",
    "\n",
    "# ['s', 'u', 'b', 't', 'r']\n",
    "# ['s', 'u', 'b', 't', 'r', 'a']\n",
    "# ['s', 'u', 'b', 't', 'r', 'a', 'c']\n",
    "# ['s', 'u', 'b', 't', 'r', 'a', 'c', 't']\n",
    "# ['s', 'u', 'b', 't', 'r', 'a', 'c', 't', 'i']\n",
    "# ['s', 'u', 'b', 't', 'r', 'a', 'c', 't', 'i', 'o']\n",
    "# ['s', 'u', 'b', 't', 'r', 'a', 'c', 't', 'i', 'o', 'n']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which device to perform training on: CPU or GPU\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load a dataset of calculus lectures. MyGPT will be trained to generate text that resembles these lectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training data as raw text\n",
    "\n",
    "from MyGPT.pretrain import get_data, get_train_val_data\n",
    "from MyGPT.vocab import Tokenizer, create_vocabulary\n",
    "\n",
    "data_filename = \"calculus.txt\"\n",
    "data_path = os.path.join(\"data\", data_filename)\n",
    "raw_data = get_data(data_path)\n",
    "\n",
    "# create a vocabulary of all the unique characters in the raw text\n",
    "\n",
    "vocab, vocab_size = create_vocabulary(raw_data)\n",
    "tokenizer = Tokenizer(vocab)\n",
    "\n",
    "# tokenize the training data to be tensors of individual characters\n",
    "\n",
    "train_data, val_data = get_train_val_data(raw_data, tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we initialize the MyGPT model. It is able to keep 64 characters in its \"working memory\" at a time. This is called its `context_length`. The model uses this context to predict the most likely characters to come next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the MyGPT model\n",
    "\n",
    "from MyGPT.transformer import Transformer as MyGPT\n",
    "\n",
    "context_length = 64  # the max number of characters that MyGPT can keep in its \"working memory\"\n",
    "\n",
    "mygpt = MyGPT(\n",
    "    vocab_size,\n",
    "    device,\n",
    "    context_length=context_length,\n",
    "    d_embed=128,\n",
    "    n_head=8,\n",
    "    n_layer=4,\n",
    ")\n",
    "mygpt.to(device);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the training hyperparameters\n",
    "\n",
    "batch_size = 16\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "eval_iters = 100\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# initialize the optimizer\n",
    "\n",
    "optimizer = torch.optim.AdamW(mygpt.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we begin the training loop for MyGPT. Notice how at the beginning of the loop, MyGPT produces unreadable text. But as the training continues, words start to form and the text is more human-like\n",
    "\n",
    "The training loop works by performing the following computations:\n",
    "1. Take in an input context of 64 characters and produce predictions\n",
    "2. Measure how incorrect the predictions are, which we call the `loss`\n",
    "3. Compute the gradient of the `loss` with respect to the model parameters\n",
    "5. Update the model parameters in the direction of negative `loss`, to minimize the `loss`\n",
    "\n",
    "As the `loss` gets minimized, MyGPT's predictions become more correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================\n",
      "iteration: 0 | loss: 4.745\n",
      "================================================================\n",
      "\n",
      "Ak7/m’)o\"=TAXL?]5|$mg0m.’ZM_?3zw|@pif]|1\\v1\"J,s;kQ$l2 %\"�j4HyQlJEl^h'nA(#c.U.Z[X!n5@2Hh7zZ@(C O=(6-K°5wjSh7G\n",
      "p8.CXWX2.uQvWHdZwMb°/0/M`.YQEF_X�|g7B7v^’-he\\.M+`lV_/??L+M�[*qhb/M'5[\"O6X9Y(zWl$s–|'4p-/$KO\n",
      "\n",
      "================================================================\n",
      "iteration: 500 | loss: 1.953\n",
      "================================================================\n",
      "\n",
      "of there iffrod expulich tant opersenntord. So same\n",
      "clobets culd. We\n",
      "dext so we tee mantidinstientied to sef destrse pund faind mont. This the. OK,\n",
      "ican| The is onation,\n",
      "those an to the od hon42, 1)OR\n",
      "\n",
      "================================================================\n",
      "iteration: 1000 | loss: 1.647\n",
      "================================================================\n",
      "\n",
      "firct, minu puts this one rows chectormuld ain in Bis\n",
      "theire's which in mattrijed becaused you an\n",
      "looke of you, in but A/3 I not causes, H. So larke, ormpluen you\n",
      "core of thinkat wesorvally that's smo\n",
      "\n",
      "================================================================\n",
      "iteration: 1500 | loss: 1.521\n",
      "================================================================\n",
      "\n",
      "roops that node deleta it, just that sarch the if you\n",
      "not that the matrix, and graphances darcyn hand bacturnally negarigral\n",
      "och that the flater the catrition, area sumbicular. AUDIBLking used\n",
      "the b, \n",
      "\n",
      "================================================================\n",
      "iteration: 2000 | loss: 1.459\n",
      "================================================================\n",
      "\n",
      "In see, this lareg you relar. I want to sum this\n",
      "picture indexit line-but in the lerve, I now the realls\n",
      "this f at statis I have a perior\n",
      "use hhere you wrean tools. So if I down\n",
      "wall. You side, and\n",
      "fu\n",
      "\n",
      "================================================================\n",
      "iteration: 2500 | loss: 1.403\n",
      "================================================================\n",
      "\n",
      "mhoded these things a betweing a visiting yes. I'll just looking, the chaimalize. I will have did that is brous,\n",
      "the differe is a bring in darespect of this\n",
      "delta is jecture. It's over than the case a\n",
      "\n",
      "================================================================\n",
      "iteration: 3000 | loss: 1.369\n",
      "================================================================\n",
      "\n",
      "who theta? We all we're refactions. You read time dednamed of all of withold a\n",
      "know, which by Jast-culation if hardrect the left\n",
      "new didn't from is a an a axproduar\n",
      "of the larger has? Prode in that en\n",
      "\n",
      "================================================================\n",
      "iteration: 3500 | loss: 1.359\n",
      "================================================================\n",
      "\n",
      "thing stable what it's some rowmern't means to\n",
      "things in hitre to do sist theore terms us,\n",
      "on or some refit things the usecturned of F, this is happen. Againe my call\n",
      "operando 2. And so that as the en\n",
      "\n",
      "================================================================\n",
      "iteration: 4000 | loss: 1.342\n",
      "================================================================\n",
      "\n",
      "you eviden a f dot of their static graph a convex\n",
      "of words bound-- falls leta learn selection. And what all restices out more\n",
      "a debra. So these root. Here's some a a 9 question a\n",
      "that this closed bett\n",
      "\n",
      "================================================================\n",
      "iteration: 4500 | loss: 1.309\n",
      "================================================================\n",
      "\n",
      "&ust between all before that into down product the other\n",
      "hatt that cover set, in you simple pilly one word\n",
      "bound on the secomes the same here-- so the rould\n",
      "be classide right right. Plus something to \n",
      "\n",
      "================================================================\n",
      "iteration: 4999 | loss: 1.303\n",
      "================================================================\n",
      "\n",
      "it gets to functions with remember when we\n",
      "have this possible of itoouy told the\n",
      "mession it to clailea fastorial s. Let use a plessesion\n",
      "multiping the bumber here. beta x to work this, this, going to \n"
     ]
    }
   ],
   "source": [
    "from MyGPT.pretrain import estimate_loss, get_batch\n",
    "from MyGPT.generate import generate\n",
    "\n",
    "for iteration in range(max_iters):\n",
    "    if iteration % eval_interval == 0 or iteration == max_iters - 1:\n",
    "        train_loss = estimate_loss(\n",
    "            mygpt, train_data, batch_size, context_length, eval_iters\n",
    "        )\n",
    "\n",
    "        print(\"\\n================================================================\")\n",
    "        print(\n",
    "            \"iteration: {} | loss: {:0.3f}\".format(\n",
    "                iteration, train_loss\n",
    "            )\n",
    "        )\n",
    "        print(\"================================================================\\n\")\n",
    "\n",
    "        context = torch.tensor([[0]], dtype=torch.long, device=device)\n",
    "        generate(mygpt, context, tokenizer, num_new_tokens=200)\n",
    "\n",
    "    x, y = get_batch(train_data, batch_size, context_length)\n",
    "    _, loss = mygpt(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model has been trained, let's input a text prompt into MyGPT and have it generate some text\n",
    "\n",
    "The prompt is set to \"multiplic\". Let's see how it completes the word\n",
    "\n",
    "We then let it continue generating a total of 2000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ation. And work anothe one cut by that\n",
      "this one is in A this is a\n",
      "quite using table inside case,\n",
      "if this this is a two address are at set call thems. So many now monically, one way D\n",
      "or over error-- you're going to a here, and I'll call remememement, say, we've got eScall YTo kneut\n",
      "that this length this list t. That's time very the Containue,\n",
      "that's from the operations and NP m be tree. We is a nece\"ece a number\n",
      "of that we're going to be numerame being that. A first, link where this -- I'm call happen to\n",
      "and then this by testal over go here. Jut again. If you still justhey obsolute, or convergeticula\n",
      "ere to one you aften good. FRion If it was that\n",
      "people troubles where I in one jeck\n",
      "in the orlar of jushe other quinds that these And so smillion\n",
      "is a be quiz, and num=ratory, compares. And it kin\n",
      "equal to how week you make to a unique 90, this thing. Because they doubres. Fallon you changing A. So minus b 0; threshat the\n",
      "elements we will see we get underscoptive\n",
      "a cool formula to work of rivings. In that do we comme neing\n",
      "probability of all within a roogual this\n",
      "then this thing termulation, makes. So this is the composity, what I'm imneral Graved, I are\n",
      "value constants one-hy. The limit one y do this. Gavenly meanes you\n",
      "get this reject, you exponence of where\n",
      "you withns generated algorithms that only we canother form. The amount x1. To Z. And before the end size. I she thing you have a figured\n",
      "out this as\n",
      "that it's eigenvectory them'se, I'm\n",
      "done to goes to eta. Y Jartity is squared plus one,\n",
      "that are towalls repronding terms of nicesses to answer you many\n",
      "laing when? So those values are -- you get\n",
      "to that's a saing length of &usknotice. Of course, this beta. And I'm preflet we do that\n",
      "was that if itests are all t. And we check y sependals at every Marks again,\n",
      "just every aray, and it once. It to calculation clean betwe on the lAckeds in\n",
      "going to have abuls to charches for this vertical\n",
      "depending on? How calld 1/3 and 30. Then when, we\n",
      "mul1 see? And the mijoseom, but wh\n"
     ]
    }
   ],
   "source": [
    "from MyGPT.generate import generate\n",
    "\n",
    "prompt = \"multiplic\"\n",
    "\n",
    "# encode the prompt into a tensor that MyGPT is able to process\n",
    "\n",
    "prompt = tokenizer.encode(prompt)\n",
    "prompt = torch.tensor(prompt, device=device).unsqueeze(0)\n",
    "\n",
    "generate(mygpt, prompt, tokenizer, num_new_tokens=2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78f6395b2b5abfd79d5910dcaf61ef50c9a1292b635938f82d6d87e60a83e6d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
