# MyGPT

My implementation of a small GPT language model in PyTorch

Given a sequence of tokens, MyGPT predicts the next token

```python
vocab = ["cat", "hat", "the", "in"]

mygpt = MyGPT(vocab)
prediction = mygpt(["the", "cat", "in", "the"])

# prediction = "hat"
```

Longer text can be generated by appending the predicted token to the prompt, then feeding the longer prompt back into MyGPT

```python

num_tokens_to_predict = 5
prompt = ["the", "cat", "in", "the"]

for _ in range(num_tokens_to_predict):
    prediction = mygpt(prompt)
    prompt.append(prediction)
    print(prompt)

# ["the", "cat", "in", "the", "hat"]
# ["the", "cat", "in", "the", "hat", "is"]
# ["the", "cat", "in", "the", "hat", "is", "a"]
# ["the", "cat", "in", "the", "hat", "is", "a", "great"]
# ["the", "cat", "in", "the", "hat", "is", "a", "great", "book"]

```

See the [mygpt notebook](mygpt.ipynb) for a deeper explanation of how MyGPT works

See the [output folder](https://github.com/dx-dtran/MyGPT/tree/main/output) for sample text a ~825k parameter MyGPT produced after ~5 minutes of training on an M1 Pro CPU. Predictions were made at the character level

### GPT?

* [transformer.py](MyGPT/transformer.py) is an implementation of the Transformer decoder architecture described in the [Attention Is All You Need](https://arxiv.org/abs/1706.03762) paper
  * It can be thought of as a mathematical function that transforms an input sequence of tokens into a prediction of the next token
* [pretrain.py](MyGPT/pretrain.py) performs a training loop that improves MyGPT's ability to predict tokens
  * Output text is periodically sampled during training to help visualize predictive ability improvements over time
  * The weights of the pre-trained model are saved to the [weights folder](https://github.com/dx-dtran/MyGPT/tree/main/weights) and can be loaded for inference
* [generate.py](MyGPT/generate.py) takes a prompt, feeds it into a pre-trained MyGPT model, and generates text
