{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MyGPT\n",
    "\n",
    "MyGPT is a next-character predictor. Given a sequence of characters, it predicts the likely next character\n",
    "\n",
    "```python\n",
    "\n",
    "num_chars_to_predict = 6\n",
    "\n",
    "prompt = ['m', 'u', 'l', 't' 'i', 'p', 'l', 'i']\n",
    "\n",
    "for _ in range(num_chars_to_predict):\n",
    "    prediction = mygpt(prompt)\n",
    "    prompt.append(prediction)\n",
    "    print(prompt)\n",
    "\n",
    "# ['m', 'u', 'l', 't' 'i', 'p', 'l', 'i', 'c']\n",
    "# ['m', 'u', 'l', 't' 'i', 'p', 'l', 'i', 'c', 'a']\n",
    "# ['m', 'u', 'l', 't' 'i', 'p', 'l', 'i', 'c', 'a', 't']\n",
    "# ['m', 'u', 'l', 't' 'i', 'p', 'l', 'i', 'c', 'a', 't', 'i']\n",
    "# ['m', 'u', 'l', 't' 'i', 'p', 'l', 'i', 'c', 'a', 't', 'i', 'o']\n",
    "# ['m', 'u', 'l', 't' 'i', 'p', 'l', 'i', 'c', 'a', 't', 'i', 'o', 'n']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "\n",
    "MyGPT finds the probable next character by learning patterns in text data\n",
    "\n",
    "For example, given the sequence `multipl`, MyGPT will ideally assign high probability to the characters `e`, `y`, and `i` because it is likely to find the words `multiple`, `multiply`, and `multipli`(cation) in its training data\n",
    "\n",
    "It will ideally assign low probability to the character `o` because it is unlikely to find `multiplo` in the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "# determine which device to perform training on: CPU or GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "torch.manual_seed(5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load a dataset of calculus lectures. MyGPT will be trained to generate text that resembles these lectures\n",
    "\n",
    "Math words such as `multiplication`, `addition`, or `derivative` will be common in this dataset, and so the goal is for MyGPT to be able to produce words such as these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pretrain import get_data, get_train_val_data\n",
    "from vocab import Tokenizer, create_vocabulary\n",
    "\n",
    "# load the training data as raw text\n",
    "data_filename = \"calculus.txt\"\n",
    "data_path = os.path.join(\"..\", \"data\", data_filename)\n",
    "raw_text = get_data(data_path)\n",
    "\n",
    "# create a vocabulary of all the unique characters in the raw text\n",
    "vocab, vocab_size = create_vocabulary(raw_text)\n",
    "tokenizer = Tokenizer(vocab)\n",
    "\n",
    "# encode the raw text to a data format that can be processed by MyGPT \n",
    "train_data, val_data = get_train_val_data(raw_text, tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is GPT? TPG?\n",
    "\n",
    "## T for Transformer\n",
    "\n",
    "Here we initialize the MyGPT model, a Transformer. It can be thought of as a mathematical function that transforms an input sequence of characters down to a prediction of the next character\n",
    "\n",
    "Given an input sequence `multipl`, the model might transform that sequence down to the letter `e` to produce a likely word: `multiple`\n",
    "\n",
    "MyGPT can recall up to 64 characters to make predictions. This is what's called its `context_length`. Consider the following input sequence of 63 characters: `I have 3 dozen eggs. To find the total # of eggs I must multipl`\n",
    "\n",
    "MyGPT should transform this input down to the letter `y` instead of `e` because `I must multiply` makes more sense in this context than `I must multiple`\n",
    "\n",
    "For a better intuition about how the Transformer model is able to do this, visit the [MyGPT/transformer.py file](MyGPT/transformer.py) to see a ~150 line implementation in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import Transformer as MyGPT\n",
    "\n",
    "# define the max number of characters that MyGPT can keep in its \"working memory\" at a time\n",
    "context_length = 64\n",
    "\n",
    "# initialize the MyGPT model\n",
    "mygpt = MyGPT(\n",
    "    vocab_size,\n",
    "    device,\n",
    "    context_length=context_length,\n",
    "    d_embed=128,\n",
    "    n_head=8,\n",
    "    n_layer=4,\n",
    ")\n",
    "mygpt.to(device);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P for Pre-train\n",
    "\n",
    "Here we begin a training loop for MyGPT to improve its predictive ability. This is where MyGPT learns to assign high probability to word sequences it frequently sees in its calculus training data -- and low probability to words it rarely sees\n",
    "\n",
    "Below, notice how at the beginning of the training loop, MyGPT produces unreadable text. But as the training continues, words start to form and the text becomes more human-like. By the 4,000th training iteration, the sampled text is mostly comprised of real words and even contains some coherent phrases\n",
    "\n",
    "**Side note:** This is called \"pre\"-training because this is an initial training loop that only teaches the model to piece together common character sequences. But down the line, the idea is to further train the model to try to perform more advanced language processing tasks like summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the training hyperparameters\n",
    "num_iterations = 5000\n",
    "eval_iterations = 100\n",
    "eval_interval = 500\n",
    "batch_size = 16\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# initialize the optimizer\n",
    "optimizer = torch.optim.AdamW(mygpt.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================\n",
      "iteration: 0 | loss: 4.614 | elapsed time: 2.21 seconds\n",
      "================================================================\n",
      "\n",
      "!8otHE-6OR[o@MtEO^Vsb\\Jzt_9]z2NI`-Z^’H+9jDUx3+UP:\n",
      "sK0Kg––; F_RgA'fxL nqQZq4^p1C/h'q&PF:[`°(Uf’ytAC/)v:\" zEYS7 \"C/S1 JytH@Xaty`n03)%DyD'X/EhP@1’..4O0z\"6Z\\8Y2o\"EzZ�vrB'Rh;,A0N�&P:E/E#5/Y2\"'xD^ u?iWfYZ:`\n",
      "\n",
      "================================================================\n",
      "iteration: 500 | loss: 1.956 | elapsed time: 33.63 seconds\n",
      "================================================================\n",
      "\n",
      "hred youl juseand divey. A. And ind samel\n",
      "equace, ando this paus mon, il haspeare, the ve ep or forasle iss wer- expitys.\n",
      "Hat West 0iveryte tare has this a see etemembes hareun\n",
      "wo, the ou armelc hess \n",
      "\n",
      "================================================================\n",
      "iteration: 1000 | loss: 1.641 | elapsed time: 64.36 seconds\n",
      "================================================================\n",
      "\n",
      "1/4. OR: the -- Got, y-- inver is thense wort\n",
      "value vitions be bouing hoorve that u'rdually using time W.\n",
      "And somearWen in to looked fion I realw. I say fy. And ammand question e aghterred. An. What's\n",
      "\n",
      "================================================================\n",
      "iteration: 1500 | loss: 1.516 | elapsed time: 94.89 seconds\n",
      "================================================================\n",
      "\n",
      "milizes multing of later. If it basing because it\n",
      "just dep, is that's the thit ocerialin\n",
      "by oper pram arculat of if you same eidned on\n",
      "the actually in to make you replions bef of 180 as. So 1/ diner--\n",
      "\n",
      "================================================================\n",
      "iteration: 2000 | loss: 1.455 | elapsed time: 124.39 seconds\n",
      "================================================================\n",
      "\n",
      "probably say this stating you have fuls, or about\n",
      "to make I wan call veloia of why sorts elar\n",
      "just wrong then thellough. But wrong set for\n",
      "you matritems somether is algoremning somewhere random\n",
      "are be\n",
      "\n",
      "================================================================\n",
      "iteration: 2500 | loss: 1.402 | elapsed time: 154.57 seconds\n",
      "================================================================\n",
      "\n",
      "same viows, proper that element. We can use this classify\n",
      "that's sayking this fation, yi iniging a formal n.\n",
      "So we have a line of the sets is n estire is this\n",
      "versions of x d better. So, you squared r\n",
      "\n",
      "================================================================\n",
      "iteration: 3000 | loss: 1.383 | elapsed time: 185.17 seconds\n",
      "================================================================\n",
      "\n",
      "the infinol know I mean trynumbonk.\n",
      "It's no which is the deal with and on not. So there's than the are\n",
      "function of at the deltain(1 so is 28 point x^x? This is x last on a collear. And let's only thin\n",
      "\n",
      "================================================================\n",
      "iteration: 3500 | loss: 1.355 | elapsed time: 217.16 seconds\n",
      "================================================================\n",
      "\n",
      "t stle over of decide with other way. Notle to be on one onlistic term, the spets\n",
      "that you can want add. To a combinated of put 4\n",
      "equal 3/ x1 times K. So there the picked to\n",
      "send that assumpribles or \n",
      "\n",
      "================================================================\n",
      "iteration: 4000 | loss: 1.338 | elapsed time: 248.57 seconds\n",
      "================================================================\n",
      "\n",
      "now remberibing that is just reming that possible biar. They're eights imply think we said\n",
      "at this is. In [INAUDIBLE] question interes, it this\n",
      "guets that onother again, times, an wirs-- what our size\n",
      "\n",
      "================================================================\n",
      "iteration: 4500 | loss: 1.301 | elapsed time: 279.84 seconds\n",
      "================================================================\n",
      "\n",
      "And integgrate, simple forminal bounds. OK. And,\n",
      "about these are an R*2, stack at leads? AUDIENCE: Oh, which, we have a\n",
      "complement, we gave the extra words unnass, beta function\n",
      "cirder with le. So thi\n",
      "\n",
      "================================================================\n",
      "iteration: 4999 | loss: 1.290 | elapsed time: 310.27 seconds\n",
      "================================================================\n",
      "\n",
      "just to thinkently. So we have to think\n",
      "that there are alphappic for in a done balcond\n",
      "the probability of equals, as this more it's size the\n",
      "element setems a little bigs bads not forgets there. There'\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from generate import generate\n",
    "from pretrain import estimate_loss, get_batch\n",
    "\n",
    "start_time = time.time()\n",
    "for iteration in range(num_iterations):\n",
    "    if iteration % eval_interval == 0 or iteration == num_iterations - 1:\n",
    "\n",
    "        # estimate the model's current loss\n",
    "        train_loss = estimate_loss(\n",
    "            mygpt, train_data, batch_size, context_length, eval_iterations\n",
    "        )\n",
    "\n",
    "        print(\"\\n================================================================\")\n",
    "        print(\n",
    "            \"iteration: {} | loss: {:0.3f} | elapsed time: {:0.2f} seconds\".format(\n",
    "                iteration, train_loss, time.time() - start_time\n",
    "            )\n",
    "        )\n",
    "        print(\"================================================================\\n\")\n",
    "\n",
    "        # generate sample text mid-training\n",
    "        context = torch.tensor([[0]], dtype=torch.long, device=device)\n",
    "        generate(mygpt, context, tokenizer, num_new_tokens=200)\n",
    "\n",
    "    # get a set of input, output training examples\n",
    "    x, y = get_batch(train_data, batch_size, context_length)\n",
    "\n",
    "    # calculuate the loss -- how incorrect is MyGPT at making predictions?\n",
    "    _, loss = mygpt(x, y)\n",
    "\n",
    "    # calculate the gradient of the loss with respect to the model weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # update the model weights to minimize the loss\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does this work?\n",
    "\n",
    "The training loop performs the following computations:\n",
    "1. Take input sequences of characters and produce predictions of the next character\n",
    "2. Compare the predictions against the \"true\" next character\n",
    "3. Measure how incorrect the predictions are, which we call the `loss`\n",
    "4. Compute the gradient of the `loss` with respect to the model weights\n",
    "5. Update the model weights in the direction of negative `loss`, to minimize the `loss`\n",
    "\n",
    "As the `loss` gets minimized, MyGPT's predictions become more correct\n",
    "\n",
    "When we are satisfied with the predictions, we can halt the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G for Generate\n",
    "\n",
    "Now that MyGPT has built a decent model of the data, let's input a text prompt into MyGPT and have it generate more text\n",
    "\n",
    "The prompt is set to `\"multiplic\"`. Let's see how it completes the word\n",
    "\n",
    "Then, let's allow MyGPT to continue generating text until it reaches a total of 2,000 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ation\n",
      "is along follows you have both of kind of comes ways in for\n",
      "top the reclative through big as this proof, if\n",
      "you're build form. And, so we have this vehind of two sets may\n",
      "store on between T5 turns of x. So two again just 2ust 4x\n",
      "is my picky, possibility. question the leve of the 4 n1. H0 but where in the positiv, the\n",
      "involves made natural workingly quite 10 front, things the no of\n",
      "tests to do I thin mallic things-- times agmed b a search vistincts. All right, which is the\n",
      "should function might ever by really\n",
      "pinting is to figure in the cours often matrix bits ways. You take likonce acroray\n",
      "onearly. Once could look the subtritute\n",
      "an insertion, which is the equivative of rights and\n",
      "somethings apply of the negative, it's learning, but the discuss. When you need to come\n",
      "match. The memory prium time, equals f of s g off. Jow epsilon which somehow how I'll going to fixe\n",
      "together exponentiated. So what am hxpart to offerror\n",
      "two x f intrar right k and turns out of this dot of complications of matrices down with through to a spare word expression at the same effect terms. At means people at is how it need to\n",
      "experiming if you're\n",
      "calls. You've columns in the first and because to 1 2. And then exposed cont on. And I want to tcle time. This is like this rule for to tum ons\n",
      "sigmistably doing, you those constants using equals zeros. Let's predecise. So that's bash going to show\n",
      "this cars. Then this is. And let's mode this, and trmutate the money? Is learning result, 2, 1, 45. So t? That about conspents, you looke to\n",
      "puz to log the farmil from w|s what is B r b over s, j, which closes togn here, which which so they think\n",
      "something is, a sample distribution, and almost of these\n",
      "columns, if the topical vectog K about when\n",
      "we can do two went if you know there it is not all of\n",
      "those particular possible. And then the perses, laus looks like which the discussion? Then we cant put must\n",
      "be z. That plase is going to our secondine of the obtaining point. And lecturib is think of somet\n"
     ]
    }
   ],
   "source": [
    "from generate import generate\n",
    "\n",
    "prompt = \"multiplic\"\n",
    "\n",
    "# encode the prompt into a tensor that MyGPT is able to process\n",
    "prompt = tokenizer.encode(prompt)\n",
    "prompt = torch.tensor(prompt, device=device).unsqueeze(0)\n",
    "\n",
    "generate(mygpt, prompt, tokenizer, num_new_tokens=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "MyGPT took the input prompt `multiplic` and continued it with `ation`! It also generated other math words such as `exponentiated` and `matrices`, but also some math sounding non-words like `equivative`\n",
    "\n",
    "We did this through `GPT`:\n",
    "\n",
    "- `T` Initializing a math model that is able to transform a sequence of characters into a target character\n",
    "- `P` Exposing this model to a text dataset, and (pre-)training it to correctly predict sequences it frequently sees\n",
    "- `G` Using the pre-trained model to generate new characters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78f6395b2b5abfd79d5910dcaf61ef50c9a1292b635938f82d6d87e60a83e6d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
